\title{Cleaning OpenStreetMap Data of Houston}
\documentclass[12pt]{article}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{graphicx}
% \usepackage{minted}
\usepackage{color}
\definecolor{lightgray}{rgb}{.7,.7,.7}
\definecolor{gray}{rgb}{.4,.4,.4}
\definecolor{darkblue}{rgb}{0,0,.3}
\usepackage[colorlinks=true,linkcolor=blue]{hyperref}
\hyphenpenalty=1000
\tolerance=3000
%  \lstdefinelanguage{XML}
% {
%   % morestring=[b][\color{red}]",
%   % morestring=[s]{>}{<},
%   % morecomment=[s]{<?}{?>},
%   stringstyle=\color{black},
%   identifierstyle=\color{darkblue},
%   keywordstyle=\color{cyan},
%   morekeywords={xmlns,version,type},
%   backgroundcolor=\color{lightgray},
%   basicstyle=\small\tt,
%   numbers=right,
%   numberstyle=\footnotesize\ttfamily\color{gray},
%   numbersep=0.5pt
% }
\begin{document}
\maketitle

\section{Introduction}
I chose Houston as my studying case, simply because right now I live here. It's the fourth largest city in America and has many immigrants, mostly of Mexican descent. The map data of Houston was obtained from Mapzen, \url{https://s3.amazonaws.com/metro-extracts.mapzen.com/houston_texas.osm.bz2}. It's named ``houton\_texas.osm'', with a size of ~540M. By sampling with ``sampling.py'', a sample file, ``sample.osm'', about 55M, was obtained. However, all operations on the full data take less than 2 minutes and $<200$M memory in my laptop, so the sample file was only used for explorations. All discussions below are for ``houton\_texas.osm''. The codes for the sample file are kept but commented out so it's straightforward if you want to switch to the sample file.

\section{Cleaning}
The cleaning codes are in ``cleaning.py''. The cleaning was carried out as follows.
\subsection{Check Tag Name}
First, I counted all tag names and attributes to check for any problematic tag and attribute name, especially for those with few counts. The output was stored in ``check\_tag\_name.md''. Everything seems to be right.

\subsection{Check v Value}
Then I examined attribute v of all tags. After exploration, some attribute k's were chosen and stored in list ``tag\_names''. Their corresponding attribute v values were stored in ``v\_values\_before\_cleaning.md''. Some problems came out at this step, as discussed below.

\subsubsection{Manual cleaning}
First of all, there are some typos and misplaced information for sure. Here are three examples.

\begin{lstlisting}[language=XML,breaklines=tr, frame=single, basicstyle=\small]
<node ...
    <tag k="addr:street" v="6111 N Ossineke Dr, Spring, TX 77386"/>
    ...
</node>
\end{lstlisting}

\begin{lstlisting}[language=XML,breaklines=tr, frame=single, basicstyle=\small]
<node ...
    <tag k="addr:housenumber" v="1200 East Blvd."/>
    ...
</node>
\end{lstlisting}

\begin{lstlisting}[language=XML,breaklines=tr, frame=single, basicstyle=\small]
<node ...
    <tag k="addr:street" v="912"/>
    <tag k="addr:housenumber" v="St. Emanuel"/>
    ...
</node>
\end{lstlisting}

In the first one, the full address rather than ``N. Ossineke Dr.'' was put in ``addr:street''. Similar mistake happened in the second case. ``addr:housenumber'' was supposed to have value of ``1200'', without street name. In the third one, the values of ``addr:street'' and ``addr:housenumber'' should be interchanged.

These certain mistakes don't have a general pattern. Fortunately, they don't occur too often, so I cleaned them by hand. For the first two types of mistakes, besides correcting the original tags, new tags like ``addr:city''and ``addr:state'' were added when necessary to keep information. All manual cleaning were summarized in ``items\_manually\_cleaned.md''. The data after manual cleaning were saved in ``houston\_texas\_manually\_cleaned.osm''. From now on, all further cleaning will be operated on this file.

\subsubsection{addr:city}
There are two types of values under tag ``addr:city''. For instance, ``Alvin'' and ``Alvin, TX'', namely, some carry the name of the state. The ``TX'' or anything similar was removed and only city names were kept.

\subsubsection{addr:housenumber}
The inconsistency here was that it could be in the form of ``111-A'', ``111 A'' or ``111 \#A''. After cleaning, all were changed to the form of ``111 \#A''.

\subsubsection{addr:postcode}
The problem of postcode was similar to that of ``addr:city''. Some postcodes were like ``TX 77009''. ``TX'' was removed in cleaning. Another problem was that some postcodes had more than five digits. For example, ``77005-1890''. I didn't change them because extra digits carried information and would be easy to truncate when necessary.

\subsubsection{addr:state}
``TX'', ``TX - Texas'', ``Texas'', ``Tx'' and ``tx'' all appeared in the data. All were replaced with ``TX''.

\subsubsection{service}
``drive-through'' was replaced with ``drive\_through'', according to OpenStreetMap's convention.\\
 \url{https://wiki.openstreetmap.org/wiki/Key:drive_through}

\subsubsection{tiger:county}
Three forms of seperator were used under this tag. They were colon: ``Austin, TX:Fort Bend, TX'', semicolon: ``Brazoria, TX;Harris, TX'', and semicolon followed by a space: ``Fort Bend, TX; Harris, TX''. All were changed to colon for consistency.

\subsubsection{addr:street}
Two main problems were found.
\begin{itemize}
\item Incomplete name, for example, ``Durham'', ``Bailey'' and ``Maroneal''. Their full name were found by searching on Google map. After cleaning by function ``update\_street\_name\_special'', they became ``Durham Drive'', ``Bailey Road'' and ``Maroneal Street''. However, there were still few of them couldn't be located because of the lack of information. I kept them after I had tried my best.
\item Over-abbreviated street names, for example, ``W Sam Houston Pky N''. It was cleaned by function ``update\_street\_name\_general'' to ``West Sam Houston Parkway North''. Similar for others.
\end{itemize}

\subsubsection{Recheck v value}
After cleaning, the data were written into ``cleaned\_file.osm'' and its v values were rechecked. It was much better.

\subsection{Transform to JSON}
The elements with name ``node'' and ``way'' in cleaned data were transformed to a JSON file, imported into MongoDB. However, a problem came up. The number of nodes plus that of ways didn't equal to the length of the entire file.

After scrutiny, it turned out that in some elements, they got a tag with $k="type"$. For them, during the transformation, key ``type'' was first assigned to ``node'' or ``way'' and later overrode by the child tag with $k="type"$. To solve it, I stored the v value of `$k="type"$ to a new key called ``k\_type'' and redid transformation. The correct JSON data was dumped into ``cleaned.json''.

\section{Data Overview}
This section contains basic statistics about the dataset and the MongoDB queries used to gather them. The JSON file was mongoimported into db.examples.houston. All queries were saved in ``mongodb\_query.py'' and results in ``MongoDB\_exploaration.md''. They are also shown below for convenience. Queries use pymongo grammar.\\
\textbf{File sizes:}\\
``houton\_texas.osm'': 541M\\
``cleaned.json'': 607.1M
\begin{itemize}
\item Length of the file:
\begin{lstlisting}[language=xml,breaklines=tr, basicstyle=\small,keywordstyle=\color{blue!70},commentstyle=\color{red!50!green!50!blue!50},frame=shadowbox, rulesepcolor=\color{red!20!green!20!blue!20}]
> db.houston.count()
2628926
\end{lstlisting}

\item Number of unique users:
\begin{lstlisting}[language=xml,breaklines=tr, basicstyle=\small,keywordstyle=\color{blue!70},commentstyle=\color{red!50!green!50!blue!50},frame=shadowbox, rulesepcolor=\color{red!20!green!20!blue!20}]
> len(db.houston.distinct("created.user"))
1053
\end{lstlisting}

\item Number of nodes:
\begin{lstlisting}[language=xml,breaklines=tr, basicstyle=\small,keywordstyle=\color{blue!70},commentstyle=\color{red!50!green!50!blue!50},frame=shadowbox, rulesepcolor=\color{red!20!green!20!blue!20}]
> db.houston.find({"type":"node"}).count()
2380486
\end{lstlisting}

\item Number of ways:
\begin{lstlisting}[language=xml,breaklines=tr, basicstyle=\small,keywordstyle=\color{blue!70},commentstyle=\color{red!50!green!50!blue!50},frame=shadowbox, rulesepcolor=\color{red!20!green!20!blue!20}]
> db.houston.find({"type":"way"}).count()
248440
\end{lstlisting}

\item Number of cafes:
\begin{lstlisting}[language=xml,breaklines=tr, basicstyle=\small,keywordstyle=\color{blue!70},commentstyle=\color{red!50!green!50!blue!50},frame=shadowbox, rulesepcolor=\color{red!20!green!20!blue!20}]
> db.houston.find({"amenity": "cafe"}).count()
72
\end{lstlisting}

\item Number of Starbucks:
\begin{lstlisting}[language=xml,breaklines=tr, basicstyle=\small,keywordstyle=\color{blue!70},commentstyle=\color{red!50!green!50!blue!50},frame=shadowbox, rulesepcolor=\color{red!20!green!20!blue!20}]
> db.houston.find({"name": "Starbucks"}).count()
40
\end{lstlisting}

\item Number of Starbucks labeled as cafe:
\begin{lstlisting}[language=xml,breaklines=tr, basicstyle=\small,keywordstyle=\color{blue!70},commentstyle=\color{red!50!green!50!blue!50},frame=shadowbox, rulesepcolor=\color{red!20!green!20!blue!20}]
> db.houston.find({"amenity": "cafe", "name": "Starbucks"}).count()
40
\end{lstlisting}

\item Top 1 contributing user:
\begin{lstlisting}[language=xml,breaklines=tr, basicstyle=\small,keywordstyle=\color{blue!70},commentstyle=\color{red!50!green!50!blue!50},frame=shadowbox, rulesepcolor=\color{red!20!green!20!blue!20}]
> db.houston.aggregate([{'$group':{'_id':'$created.user', 'count':{'$sum':1}}}, {'$sort':{'count':-1}}, {'$limit': 1}]):
{u'count': 659497, u'_id': u'woodpeck_fixbot'}
\end{lstlisting}

\item Top 10 appearing restaurants:
\begin{lstlisting}[language=xml,breaklines=tr, basicstyle=\small,keywordstyle=\color{blue!70},commentstyle=\color{red!50!green!50!blue!50},frame=shadowbox, rulesepcolor=\color{red!20!green!20!blue!20}]
> db.houston.aggregate([{"$match":{"cuisine":{"$exists":1},"amenity":'restaurant'}},{'$group':{'_id':'$cuisine', 'count':{'$sum':1}}}, {'$sort':{'count':-1}}, {'$limit': 10}]):
{u'count': 47, u'_id': u'mexican'}
{u'count': 28, u'_id': u'american'}
{u'count': 19, u'_id': u'italian'}
{u'count': 15, u'_id': u'pizza'}
{u'count': 15, u'_id': u'burger'}
{u'count': 13, u'_id': u'chinese'}
{u'count': 9, u'_id': u'seafood'}
{u'count': 8, u'_id': u'steak_house'}
{u'count': 7, u'_id': u'barbecue'}
{u'count': 5, u'_id': u'vietnamese'}
\end{lstlisting}

\item Number of ATMs:
\begin{lstlisting}[language=xml,breaklines=tr, basicstyle=\small,keywordstyle=\color{blue!70},commentstyle=\color{red!50!green!50!blue!50},frame=shadowbox, rulesepcolor=\color{red!20!green!20!blue!20}]
> db.houston.find({"amenity": "atm"}).count()
12
\end{lstlisting}

\item Number of banks:
\begin{lstlisting}[language=xml,breaklines=tr, basicstyle=\small,keywordstyle=\color{blue!70},commentstyle=\color{red!50!green!50!blue!50},frame=shadowbox, rulesepcolor=\color{red!20!green!20!blue!20}]
> db.houston.find({"amenity": "bank"}).count()
188
\end{lstlisting}

\item Top 10 appearing amenities:
\begin{lstlisting}[language=xml,breaklines=tr, basicstyle=\small,keywordstyle=\color{blue!70},commentstyle=\color{red!50!green!50!blue!50},frame=shadowbox, rulesepcolor=\color{red!20!green!20!blue!20}]
> db.houston.aggregate([{"$match":{"amenity":{"$exists":1}}},{'$group':{'_id':'$amenity', 'count':{'$sum':1}}}, {'$sort':{'count':-1}}, {'$limit': 10}]):
{u'count': 2485, u'_id': u'parking'}
{u'count': 2435, u'_id': u'place_of_worship'}
{u'count': 1659, u'_id': u'school'}
{u'count': 669, u'_id': u'fast_food'}
{u'count': 466, u'_id': u'restaurant'}
{u'count': 429, u'_id': u'fountain'}
{u'count': 402, u'_id': u'fire_station'}
{u'count': 327, u'_id': u'fuel'}
{u'count': 209, u'_id': u'grave_yard'}
{u'count': 188, u'_id': u'bank'}
\end{lstlisting}
\end{itemize}

\section{Discussions}
\begin{itemize}
\item More than half of cafes reported are Starbucks. They're just so popular.
\item The number of ATMs is much less than that of banks, but from daily life experience we know that a bank has at least one ATM, so it can't be true. People are more reluctant to upload information of ATM, probably because they usually don't think of ATMs as being amenities.
\item  The most popular food is Mexican. Quite fair. We are in Texas!
\end{itemize}

\section{Additional Thoughts}
In the data wrangling, the most annoying step was manual cleaning, which was the straightforward solution for those ``hard'' mistakes, like typos and obviously misplaced information. They could be solved programmatically, like using regex, but the codes would be hardly portable because of the randomness of such mistakes. As the result, I didn't bother to do so. Though such mistakes were only a few but they asked for lots of efforts. It would be a good idea if OpenStreetMap could do some prescreening. For example, they could check for the type of postcode to see if it's a number. A potential problem of prescreening is that there might be some special cases that don't obey the constraints but are still right. As the world is large, it's better to keep some space for special cases. The compromise can be that when an unusual input comes in, a warning would be thrown, but it's not mandatory. It just reminds the user to double-check the input. This simple procedure could significantly reduce unconscious errors, and save data scientist's life.

\section{Notes}
Some codes are commented out in the final version. To repeat the results above. First you need to extract those zipped files in the directory. Then you can checkout the corresponding commits:\\
33b38a1310479872bebe116c616a7f39450aa4ca (check tag names)\\
e6ec0006052a459216499195047fb5cd0286ad44 (check v values)\\
791cc75287c2ef8645ac32f148d1195de167177b (check v values after cleaning)\\
72b1b3c88b19a525c88cbf241af25d981822ce4d (write cleaned data to an osm file)\\
c4dbfe210f2ee42050def47c7382ffcfc9590be7 (write cleaned data to an JSON file).
\end{document}

